[DATA]
; Define the column(s) to perform one-hot encoding to.
; Options: [blank] (no one-hot encoding), 'day_of_week', 'lagged_day_of_week', etc.
; !!! CHANGING THIS VALUE WILL REQUIRE THE CACHE TO BE REBUILT.
; str
COLUMNS_ONE_HOT_ENCODING = day_of_week, lagged_day_of_week

; Define the column name(s) for all feature columns (X).
; *: Use all columns with the exception of those defined in COLUMNS_Y below. This is the default behavior.
; !!! CHANGING THIS VALUE WILL REQUIRE THE CACHE TO BE REBUILT.
; str
COLUMNS_X = *

; Define the column name(s) of the label (y).
; !!! CHANGING THIS VALUE WILL REQUIRE THE CACHE TO BE REBUILT.
; str
COLUMNS_Y = close_greater_than_open

; Define the total number of days to holdout for the test set.
; 0: Disabled, train the model on the entire training set for production
; int
HOLDOUT_DAYS = 180

; Choose symbols to process. Leave blank to process all symbols in the input file.
; Example: SYMBOLS = AAPL, MSFT, GOOGL
; str
SYMBOLS = AAPL

[FEATURE SELECTION]
;-------------------
;--- SELECTKBEST ---
;-------------------
; Enable SelectKBest.
; 0: Disabled
; 1: Enabled
; bool
ENABLE_SELECTKBEST = 1

; Set the K parameter.
; int
SELECTKBEST_K = 45

;-------------
;--- RFECV ---
;-------------
; Enable Recursive Feature Elimination with Cross-Validation.
; 0: Disabled
; 1: Enabled
; bool
ENABLE_RFECV = 1

; RFECV step size. From https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html:
; "If greater than or equal to 1, then step corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of features to remove at each iteration. Note that the last iteration may remove fewer than step features in order to reach min_features_to_select."
; int or float
RFECV_STEP_SIZE = 2

;--------------------------
;--- VARIANCE THRESHOLD ---
;--------------------------
; Enable VarianceThreshold.
; 0: Disabled
; 1: Enabled
; bool
ENABLE_VARIANCE_THRESHOLD = 1

; Set the threshold parameter.
; float
VARIANCE_THRESHOLD_THRESHOLD = 0.01

[GENERAL]
; Directory containing cache files such as pickle files containing imported and already processed data.
; str
CACHE_DIRECTORY = cache

; Choose whether cross-validation will be performed.
; 0: Disabled
; 1: Enabled
; bool
PERFORM_CROSS_VALIDATION = 1

; Choose whether feature selection will be performed.
; 0: Disabled
; 1: Enabled
; bool
PERFORM_FEATURE_SELECTION = 1

; Choose whether GridSearchCV will be performed for hyperparameter optimization.
; 0: Disabled
; 1: Enabled
; bool
PERFORM_HYPERPARAMETER_OPTIMIZATION = 0

; Random seed(s) for reproducibility.
; If multiple seeds are provided, the program will iterate through each seed and aggregate the results.
; int
RANDOM_SEED = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25

; Save the scores for every learner to an output file.
; 0: Disabled
; 1: Enabled
; bool
SAVE_RESULTS_TO_FILE = 0

[ML]
; Total number of folds to use in cross-validation. This setting is used anywhere CV is performed (e.g, feature selection).
; 0: Disabled
; int
CROSS_VALIDATION_FOLDS = 5

; The number of days the model predicts before incorporating the actual results into the training set and re-fitting. A value of 5 simulates a weekly re-training schedule.
; int
RETRAIN_STEP_FREQUENCY = 20

; The accuracy of the model must be greater than or equal to the set threshold in order for the model to be saved.
; -1: Disabled
; float (0.0 to 1.0): Enable saving models that meet or exceed the threshold.
SAVE_THRESHOLD = -1

; The scoring metric to use from scikit-learn. Specify the entire module path so it can be dynamically loaded.
; str
SCORING_METRIC = sklearn.metrics.matthews_corrcoef

; Specify all parameters for the scoring metric defined above.
; json or {empty}
SCORING_METRIC_PARAMETERS = {}

; Number of historical rows to use for retraining. 
; 0: Expanding window (all history)
; 500: Sliding window (approx 2 years of daily data)
; int
SLIDING_WINDOW_SIZE = 250

; List of all learners to use.
; Use the format: 'Model0, Model1, Model2'. To train all available models, use: 'ALL'
; str
USE_LEARNERS = Decision Tree, Logistic Regression, Random Forest, SVC

[NORMALIZATION]
; Choose which type of normalization to perform.
; Options: 'MinMaxScaler', 'RobustScaler', 'StandardScaler'
; str
NORMALIZE_METHOD = StandardScaler
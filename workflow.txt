I have historical stock data for the past 2 years, which provides daily data. My task is to predict whether the closing price of a stock will be greater than the opening price (class = 1), or not (class = 0). As an overview, my workflow is to first obtain all data from Polygon.io, specifically daily data. This provides the following features for every symbol:

	a) Opening price (‘o’)
	b) Closing price (‘c’)
	c) The trading volume for the given time period (‘v’)
	d) Highest price for the given time period (‘h’)
	e) Lowest price for the given time period (‘l’)
	f) Number of transactions within the aggregate window (’n’)
	g) Volume weighted average price ('vw')


For a given symbol, training is performed using the current day's opening price and lagged features from previous days, up to 15 days total.

Next, features are added such as the day of the week (e.g., ‘Monday’, ‘Tuesday’, etc.) and the class labels for whether the closing price is greater than the opening price, or not. Additionally, technical indicators (i.e., moving averages, momentum, rate of change, volatility, rolling min, max, and range, and z-score of returns), candlestick features (i.e., candleand relative body, upper and lower shadow), interaction features (i.e., close, return, and spread volume), and calendar features (i.e., month, quarter, day of month, cyclical encoding for day of week (7-day cycle), cyclical encoding for day of month (variable-length cycle)), and the lagged high-low spread and the lagged daily return for all previous days were also calculated. Finally, any row was NaNs are removed entirely rather than replacing them. The data is now saved to a CSV file.

Before classification can be performed, preprocessing of the data is completed. The data is then split into training and test sets, where the last 120 days are used as the test set while every other day is used for training. One-Hot encoding is then performed on features with string values such as the day of the week. Normalization is then performed, where the training and test sets are normalized separately. Here, the fitted scaler from the training set is used for the test set.

It's now time for machine learning (ML). Performance is averaged over 25 random seeds to get a robust score. Within each random seed, hyperparameter optimization via RandomizedSearchCV is used to identify the best hyper-parameters for each of the following classifiers: Logistic Regression, Decision Tree, SVC. Here, TimeSeriesSplit() with the f1_macro scoring function was used for 5-fold CV to identify the best model. The best model is then used in CV to calculate its CV score and standard deviation. Again 5-fold TimeSeriesSplit with the f1_macro scoring function was used. Finally, the best model was used on the test set to get the final performance. The score function used was f1_macro. After all seeds are done I average the scores over all seeds.
